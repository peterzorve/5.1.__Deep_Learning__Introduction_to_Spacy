{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import spacy as sp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = sp.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> An atom is the smallest unit of ordinary matter that forms a chemical element.[1] Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms\n"
     ]
    }
   ],
   "source": [
    "document = \"An atom is the smallest unit of ordinary matter that forms a chemical element.[1] Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms\"\n",
    "print(type(document), document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'> An atom is the smallest unit of ordinary matter that forms a chemical element.[1] Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms\n"
     ]
    }
   ],
   "source": [
    "document = nlp(document)\n",
    "print(type(document), document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An\n",
      "atom\n",
      "is\n",
      "the\n",
      "smallest\n",
      "unit\n",
      "of\n",
      "ordinary\n",
      "matter\n",
      "that\n",
      "forms\n",
      "a\n",
      "chemical\n",
      "element.[1\n",
      "]\n",
      "Every\n",
      "solid\n",
      ",\n",
      "liquid\n",
      ",\n",
      "gas\n",
      ",\n",
      "and\n",
      "plasma\n",
      "is\n",
      "composed\n",
      "of\n",
      "neutral\n",
      "or\n",
      "ionized\n",
      "atoms\n"
     ]
    }
   ],
   "source": [
    "for i in document:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An atom is the smallest unit of ordinary matter that forms a chemical element.[1]\n",
      "Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms\n"
     ]
    }
   ],
   "source": [
    "for i in document.sents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_context', '_get_array_attrs', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'copy', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_dict', 'from_disk', 'from_docs', 'get_extension', 'get_lca_matrix', 'has_annotation', 'has_extension', 'has_unknown_spaces', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'noun_chunks', 'noun_chunks_iterator', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_ents', 'set_extension', 'similarity', 'spans', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_dict', 'to_disk', 'to_json', 'to_utf8_array', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
     ]
    }
   ],
   "source": [
    "print(dir(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An atom is the smallest unit of ordinary matter that forms a chemical element.[1]\n",
      "Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms\n"
     ]
    }
   ],
   "source": [
    "for i in document.sents:\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'thereafter', 'you', 'against', 'fifteen', 'enough', 'their', 'it', '’s', 'forty']\n",
      "['all', 'thereafter', 'you', 'against', 'fifteen', 'enough', 'their', 'it', '’s', 'forty']\n"
     ]
    }
   ],
   "source": [
    "stop_word = sp.lang.en.STOP_WORDS\n",
    "print(list(stop_word)[:10])\n",
    "\n",
    "spacy_stopwords = sp.lang.en.stop_words.STOP_WORDS\n",
    "print(list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "document = \"An atom is the smallest unit of ordinary matter that forms a chemical element. Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms\"\n",
    "print(type(document))\n",
    "\n",
    "document = nlp(document)\n",
    "print(type(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['atom', 'smallest', 'unit', 'ordinary', 'matter', 'forms', 'chemical', 'element', '.', 'solid', ',', 'liquid', ',', 'gas', ',', 'plasma', 'composed', 'neutral', 'ionized', 'atoms']\n"
     ]
    }
   ],
   "source": [
    "document_list = [token.text for token in document if not token.is_stop]\n",
    "print(document_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1. re.findall(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['131231', '9449', '83782']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \" This is a string that contains some 131231 random 9449 number in 83782 it.\"\n",
    "\n",
    "numbers = re.findall('\\d+', text)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '2', '1']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "There are 3\n",
    "Numbers in this (2 remaining)\n",
    "text. Still 1 is missing.\n",
    "\"\"\"\n",
    "\n",
    "pattern = r'\\d+'\n",
    "re.findall(pattern, text, re.MULTILINE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2. re.search(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['131231', '9449', '83782']\n",
      "Matched\n"
     ]
    }
   ],
   "source": [
    "pattern = '\\d+'\n",
    "text = \" This is a string that contains some 131231 random 9449 number in 83782 it.\"\n",
    "numbers = re.findall(pattern, text)\n",
    "print(numbers)\n",
    "\n",
    "pattern = 'contaiN'\n",
    "if re.search(pattern, text, re.IGNORECASE):\n",
    "    print('Matched')\n",
    "else:\n",
    "    print('Not matched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 42 72 75\n",
      "['__class__', '__copy__', '__deepcopy__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'end', 'endpos', 'expand', 'group', 'groupdict', 'groups', 'lastgroup', 'lastindex', 'pos', 're', 'regs', 'span', 'start', 'string']\n"
     ]
    }
   ],
   "source": [
    "text = \" This is a string that contains some 131231 random 9449 number in 83782 it.\"\n",
    "print(text.find('1'), text.find('1 '), text.find('it'), len(text))\n",
    "\n",
    "pattern = \"\\d+\"\n",
    "\n",
    "match = re.search(pattern, text)\n",
    "print(dir(match))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `3. Examples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match.end         = 43\n",
      "match.endpos      = 75\n",
      "match.group       = 131231\n",
      "match.groupdict   = {}\n",
      "match.groups      = ()\n",
      "match.lastgroup   = None\n",
      "match.lastindex   = None\n",
      "match.pos         = 0\n",
      "match.re          = re.compile('\\\\d+')\n",
      "match.regs        = ((37, 43),)\n",
      "match.span        = (37, 43)\n",
      "match.start       = 37\n",
      "match.string      =  This is a string that contains some 131231 random 9449 number in 83782 it.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a string that contains some 131231 random 9449 number in 83782 it.\"\n",
    "\n",
    "print('match.end         =', match.end())\n",
    "print('match.endpos      =', match.endpos)\n",
    "print('match.group       =', match.group())\n",
    "print('match.groupdict   =', match.groupdict())\n",
    "print('match.groups      =', match.groups())\n",
    "print('match.lastgroup   =', match.lastgroup)\n",
    "print('match.lastindex   =', match.lastindex)\n",
    "print('match.pos         =', match.pos)\n",
    "print('match.re          =', match.re)\n",
    "print('match.regs        =', match.regs)\n",
    "print('match.span        =', match.span())\n",
    "print('match.start       =', match.start())\n",
    "print('match.string      =', match.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31231 \n"
     ]
    }
   ],
   "source": [
    "start, end = match.span()\n",
    "char = text[start:end]\n",
    "print(char)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `4. re.match(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern is found at 37 position\n",
      "pattern not found\n"
     ]
    }
   ],
   "source": [
    "pattern = 'This'\n",
    "text1  = 'this is an example'\n",
    "text2  = 'and this is an example'\n",
    "\n",
    "if re.match(pattern, text1, re.IGNORECASE):\n",
    "    print('pattern is found at', match.start(), 'position')\n",
    "else:\n",
    "    print('pattern not found')\n",
    "\n",
    "if re.match(pattern, text2):\n",
    "    print('pattern is found at', match.start(), 'position')\n",
    "else:\n",
    "    print('pattern not found')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `5. re.split(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sentence', ' This is another sentence', ' End']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This is a sentence', ' This is another sentence', ' End']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a sentence. This is another sentence. End\"\n",
    "print(text.split(\".\"))\n",
    "\n",
    "pattern = '\\.'\n",
    "re.split(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a sentence', 'This is another sentence', 'End']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"This is a sentence \\n\n",
    "        -------------------- \\n\n",
    "        This is another sentence. \\n\n",
    "        ------------------------- \\n\n",
    "        End\"\"\"\n",
    "\n",
    "pattern = \"\\s*-+\\s*|\\.*\\s*\\n\\W*\" \n",
    "\n",
    "re.split(pattern, text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Preprocess TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "     with open(path, 'r') as f:\n",
    "          lines = f.readlines()\n",
    "          for i in range(len(lines)):\n",
    "               lines[i] = lines[i].replace('.', '').replace(',', '').replace(':', '').replace(';', '').replace('?', '').replace('!', '').replace('-', '').replace('_', '').replace('\\'', '').replace('\\\"', '').lower()\n",
    "          with open('./data/article_processed.txt', 'w') as p:\n",
    "               p.writelines(lines)\n",
    "     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(path):\n",
    "     f = open(path, 'r')\n",
    "     lines = f.readlines()\n",
    "     sentences = [line.split() for line in lines]\n",
    "     f.close()\n",
    "     return sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(sentences):\n",
    "     i = 0 \n",
    "     while i <  len(sentences):\n",
    "          if sentences[i] == []:\n",
    "               sentences.pop(i)\n",
    "          else: \n",
    "               i += 1 \n",
    "     return sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences('data/article_processed.txt')\n",
    "cleaned_sents = clean_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(sentences):\n",
    "    vocab = []\n",
    "    for sentence in sentences: \n",
    "        for token in sentence:\n",
    "            if token not in vocab:\n",
    "                vocab.append(token)\n",
    "    word2idx = {word : idx  for (idx, word) in enumerate(vocab)}\n",
    "    idx2word = {idx  : word for (idx, word) in enumerate(vocab)}\n",
    "\n",
    "    return word2idx, idx2word, len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx, idx2word, len_vocab = get_dictionary(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(sentences, word2idx, r):\n",
    "    pairs = []\n",
    "    for sentence in sentences: \n",
    "        tokens = [word2idx[word] for word in sentence]\n",
    "\n",
    "        for center in range(len(tokens)):\n",
    "            for context in range(-r, r+1):\n",
    "                context_word = center + context\n",
    "\n",
    "                if context_word < 0 or context_word >= len(tokens) or context_word == center:\n",
    "                        continue \n",
    "                else: \n",
    "                        pairs.append( (tokens[center], tokens[context_word]) )\n",
    "    return np.array(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[  0,   1],\n",
      "       [  0,   2],\n",
      "       [  0,   3],\n",
      "       ...,\n",
      "       [335, 333],\n",
      "       [335, 334],\n",
      "       [335,  97]]), 336)\n"
     ]
    }
   ],
   "source": [
    "def get_dataset():\n",
    "    sentences = get_sentences('data/article_processed.txt')\n",
    "    clean_sents = clean_sentences(sentences)\n",
    "    word2idx, idx2word, len_vocab = get_dictionary(clean_sents)\n",
    "    pairs = get_pairs(clean_sents, word2idx, 4)\n",
    "\n",
    "    return pairs, len_vocab\n",
    "\n",
    "print(get_dataset())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
